# Documentation for AI Project Dashboard – LLM RAG Approach

## 1. Architecture for the LLM RAG Approach & Detailed Methodology

### Overview
Our AI Project Dashboard integrates multiple AI applications including regression, clustering, and neural network modules. The focus of this documentation is on the Large Language Model (LLM) Q&A module which utilizes a Retrieval-Augmented Generation (RAG) approach. In our RAG pipeline, user queries are augmented with contextual information drawn from domain-specific documents. This additional context improves the specificity and reliability of the generated answers.

### Architectural Components

- **User Interface (UI):**  
  Built using Streamlit, the UI consists of multiple tabs (Regression, Clustering, Neural Network, and LLM Q&A). For the LLM RAG module, the UI provides:
  - A dropdown for document selection (Academic City Student Handbook or 2025 Budget Statement).
  - A text input box for user queries.
  - An input for maximum output token customization.
  - Real-time response display.

- **Document Retrieval & Preprocessing:**  
  - **PDF Extraction:** The selected document (PDF) is read from disk and processed using the PyPDF2 package.
  - **Chunk Splitting:** The document text is split into manageable chunks (approximately 500 words each) to stay within the model’s context window.
  - **Similarity Retrieval:** A TF-IDF vectorizer computes cosine similarity between the user’s query and each chunk. The most relevant chunk is selected as the context for the answer.

- **Prompt Construction & Generation:**  
  - The retrieved context snippet and the user’s question are concatenated into a prompt.  
  - A configuration object (GenerateContentConfig) specifies parameters such as maximum output tokens, temperature, top_p, and top_k.  
  - The prompt and configuration are sent to Google’s Gemini API (model "gemini-2.0-flash-001") via the Google Gen AI SDK, which generates the answer in real time.

- **Output Presentation:**  
  - The generated answer is displayed on the UI along with a snippet of the contextual document to inform the user of the source of information.
  
### Methodology in Detail
1. **Context Extraction:**  
   - When a user selects a document, the system loads the full PDF locally.
   - The text is extracted using PyPDF2 and then split into smaller chunks (using a simple word-based split).

2. **Contextual Retrieval:**  
   - A TF-IDF vectorizer is employed to represent each chunk as a vector.
   - Cosine similarities between the user query and each chunk are computed.
   - The chunk with the highest similarity score is selected as the relevant context.

3. **Prompt Formation & Generation:**  
   - The system then formats a prompt that begins with a label showing the chosen context (including a snippet preview) and appends the user's question.
   - The prompt, along with a generation configuration (which includes user-defined max output tokens), is sent to the Gemini API.
   - The API processes this input and generates a coherent answer based on both the context and the query.

4. **Result Display & User Feedback:**  
   - The generated response is then displayed in the application.
   - Additional metadata (e.g., similarity score for the retrieved context) is also shown to provide transparency to the user about how the answer was grounded.

---

## 2. Clear Instructions on How to Use Each Feature

### Regression Module
1. **Upload CSV File:**  
   - Use the file uploader to select a CSV containing your regression dataset.
2. **Select Target and Feature Columns:**  
   - Choose the target column for predictions; remaining numeric columns are treated as features.
3. **View EDA:**  
   - Expand the EDA section to view descriptive statistics, correlation matrices, box plots, and scatter plots.
4. **Model Training and Evaluation:**  
   - The system automatically trains a linear regression model, displaying metrics (MAE, MSE, RMSE, R²) and plots (predictions vs. actual, residuals).
5. **Custom Prediction:**  
   - Enter new data points via the provided input boxes to generate a custom prediction.

### Clustering Module
1. **Upload CSV File:**  
   - Use the uploader to load your clustering dataset.
2. **Select Features:**  
   - Choose numeric columns for clustering.
3. **Optional Steps:**  
   - Toggle options for feature scaling, outlier removal, and advanced evaluation.
4. **View Clusters:**  
   - The module displays evaluation metrics and visualizations (2D scatter or PCA plots).
5. **Download Results:**  
   - Download the clustered data as a CSV file.

### Neural Network Module
1. **Upload CSV File:**  
   - Provide your classification dataset.
2. **Select Target and Features:**  
   - Choose which column is the classification target and which numeric columns to use as features.
3. **Model Training:**  
   - Set hyperparameters (epochs, learning rate, batch size).  
   - The system trains a neural network (using TensorFlow) and shows a model summary and training progress.
4. **Prediction:**  
   - Input values to test the trained model and view the prediction and probability distribution.

### LLM Q&A Module with RAG (RAG Approach)
1. **Select Context Document:**  
   - Use the dropdown to choose between the Academic City Student Handbook and the 2025 Budget Statement.
2. **Context Loading and Preview:**  
   - The system loads the selected PDF from local storage, extracts text, and displays a snippet (first 500 words) along with the document’s label.
3. **Enter Query:**  
   - Type your question in the provided text input.
4. **Retrieval and Generation:**  
   - The document is split into chunks, and the most relevant chunk is retrieved using TF-IDF and cosine similarity.
   - A prompt is built using the retrieved chunk and your query.
   - The Gemini API is called with a configuration that includes a user-set maximum token value.
5. **View Answer:**  
   - The generated answer is displayed in real time.

---

## 3. Detailed Description of Datasets and Models Used (LLM RAG)

### Datasets
- **Academic City Student Handbook:**  
  Contains policies, procedures, and various academic guidelines provided to students. It includes detailed rules, curricular information, and administrative policies.
- **2025 Budget Statement and Economic Policy:**  
  A governmental document detailing the 2025 budget, economic forecasts, performance metrics, and policy initiatives in Ghana for the upcoming financial year.

*Note:* Both documents are provided as PDF files stored locally, and their text is extracted for use as context in the RAG approach.

### Models Used
- **Google Gemini API (LLM):**  
  We leverage the Gemini API via the Google Gen AI SDK. The chosen model (e.g., "gemini-2.0-flash-001") is a pre-trained large language model used for generative tasks.
- **Other Modules’ Models:**  
  - **Regression:** Uses scikit-learn’s `LinearRegression`.
  - **Clustering:** Uses scikit-learn’s `KMeans`.
  - **Neural Network:** Built with TensorFlow/Keras.

The LLM part is the focus here, and it uses a text generation model that produces responses by leveraging context from the selected document.

---

## 4. Detailed Architecture for the LLM RAG Approach

### System Architecture Diagram (Conceptual)
Look at LLM Q&A RAG Architecture.svg


### Methodology
1. **Document Processing:**  
   - The selected PDF is parsed, and its text is extracted.
   - The document is split into manageable chunks.
2. **Context Retrieval:**  
   - A TF-IDF vectorizer represents each chunk.
   - Cosine similarity identifies the chunk most relevant to the user’s query.
3. **Prompt Formation:**  
   - The retrieved chunk is combined with the user’s query to create a comprehensive prompt.
4. **Generation:**  
   - The Gemini API is called with this prompt and configuration settings (including user-defined max tokens).
5. **Output:**  
   - The generated answer is displayed, and metadata (like the context snippet and similarity score) is shown to inform the user.

---

## 5. Evaluation, Analysis, and Comparison with ChatGPT

### Evaluation and Analysis
- **Quality and Relevance:**  
  The RAG approach grounds the generated answer in domain-specific context. This means that, for queries that rely on detailed policy or budget information, the responses tend to be more accurate and relevant than what a generic model (like ChatGPT) might produce.
- **Transparency:**  
  Our system shows a snippet of the context used, along with a similarity score. This transparency allows users to understand exactly which part of the document was used to generate the response.
- **Customization:**  
  With parameters like maximum output tokens adjustable by the user, our system offers fine control over answer length, which can be tuned based on the specificity of the query.

### Comparison with ChatGPT
- **ChatGPT Approach:**  
  ChatGPT (e.g., GPT-3.5-turbo or GPT-4) generates responses based on its internal knowledge base without explicit external context. While it is generally fluent and versatile, its responses for highly domain-specific queries may be less precise.
- **Our RAG Approach:**  
  By explicitly retrieving relevant context from Academic City policies or budget documents, our system is designed to produce answers that are both accurate and specific to the domain. This leads to improved precision in domains where up-to-date or authoritative information is critical.
- **Response Transparency and Trust:**  
  Our approach provides the user with insights (e.g., context snippet, similarity scores) into how the answer was derived, which is often missing in standard ChatGPT responses where the internal reasoning is hidden.
- **Use Case Suitability:**  
  For tasks requiring detailed comprehension of a specific text (such as policy interpretation or budget analysis), the RAG method typically outperforms a generic model by ensuring that the generated output is directly informed by the source document.

  **example**
  **Retrieved context snippet (most relevant chunk):

the prior and written consent of the Resident hostel coordinator to whom an application should be presented at least 5 working days in advance. e) Students mu st vacate the hostels during their vacations unless they are taking courses or unless an end-of-semester project or exam due date is extended. All students will be given up to a week after exams to prepare and leave the hostel. International students resid ing outside Ghana must make a written request to the Students and Community Affairs  ...
Similarity score: 0.15

Answer:

Based on the provided text, alcohol is not allowed in the Academic City hostel. Section "d) Smoking, alcohol and illegal drugs are strictly prohibited." under the Restrictions section explicitly forbids alcohol.**



### Conclusion
Our RAG approach leverages domain-specific context to fine-tune the generative output, thereby offering more targeted and reliable answers for user queries. While ChatGPT is powerful for general inquiries, the retrieval-augmented method provides enhanced relevance and trustworthiness for our specialized use cases.

---



